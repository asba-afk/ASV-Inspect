================================================================================
                          PROJECT REPORT
           ASV-INSPECT: AI-Based Automated Assembly Verification System
================================================================================

Chapter 6
ASV-INSPECT - Automated Statistical Verification for Assembly Inspection

6.1 Introduction

ASV-INSPECT is an AI-powered computer vision system designed to automate the inspection and verification of mechanical assemblies in manufacturing environments. The system uses YOLOv8 deep learning for component detection and DBSCAN clustering for statistical modeling to identify missing parts in gearbox assemblies. It addresses critical quality control challenges where manual inspection was time-consuming (2-5 minutes per assembly) and error-prone (2-3% defect rate), reducing inspection time by 90% while achieving 95.6% accuracy.

6.2 Objectives

• Automate mechanical assembly inspection with AI-powered component detection
• Achieve >90% detection accuracy (mAP50) for bolts, bearings, and oil jets
• Provide real-time processing under 500ms per assembly
• Create intuitive interfaces (Web UI, REST API) for inspectors and system integration
• Implement statistical learning for manufacturing variation tolerance
• Generate detailed visual and JSON reports with compliance scoring

6.3 Key Features

• Multi-Class Detection – YOLOv8 detects 3 component types with 94% mAP50 accuracy
• Statistical Golden Model – DBSCAN clustering learns expected positions (18 components: 12 bolts, 4 bearings, 2 oil jets)
• Dual Inspection Modes – Position-based (WHERE missing) and Count-only (HOW MANY missing)
• Real-Time Processing – 200-500ms per inspection
• Web Interface – Streamlit UI with drag-and-drop, real-time adjustments
• REST API – FastAPI integration for production lines
• Adaptive Tolerance – Auto-adjusts based on component variance
• Visual Reports – Annotated images with green boxes (detected), red circles (missing), PASS/FAIL banners
• Batch Processing – Multi-assembly inspection with aggregate statistics

6.4 System Architecture Overview

ASV-INSPECT follows a multi-layered architecture designed for scalability and modularity:

ARCHITECTURE LAYERS:

1. DATA LAYER – 148 labeled images, 2,454 annotations (3 classes: bolt, bearing, oil jet)
2. TRAINING LAYER – YOLOv8 training (94% mAP50, 11.3hr CPU) + DBSCAN golden model generation
3. MODEL LAYER – best.pt (6.2MB detector) + golden_model.json (18 expected components with variance)
4. INFERENCE LAYER – Assembly Inspector with nearest neighbor matching, count-only mode, adaptive tolerance
5. OUTPUT LAYER – Annotated images (green boxes/red circles), JSON reports, PASS/FAIL status
6. APPLICATION LAYER – Streamlit UI, FastAPI REST API, CLI tools, batch processor

DATA FLOW:
Training: Images+Labels → YOLO → best.pt | Images+Labels → DBSCAN → golden_model.json
Inspection: Image → YOLO → Components → Matching → Missing Detection → Report → PASS/FAIL

6.5 Advantages

• Dramatically Reduces Inspection Time: From 2-5 minutes manual inspection to <1 second automated processing (90%+ reduction).
• Eliminates Human Error: Consistent 95.6% accuracy vs. 2-3% manual error rate.
• Handles Natural Variation: Statistical golden model adapts to manufacturing tolerances without requiring perfect alignment.
• Real-Time Feedback: Instant visual confirmation with specific missing component identification.
• Scalable Architecture: Single machine can handle 170-390 inspections per minute (CPU/GPU), supporting 85-195 inspection stations.
• Cost-Effective Deployment: Runs on standard hardware (CPU-only viable), no expensive specialized equipment required.
• Flexible Integration: Multiple interfaces (Web UI, REST API, CLI) for various deployment scenarios.
• Minimal Training Required: Intuitive interface enables operator proficiency in 15 minutes.
• Detailed Documentation: Visual and JSON reports provide audit trail and quality metrics.
• Extensible Design: Modular architecture allows easy addition of new component types or assembly variants.
• Production-Ready: Handles varying lighting conditions, multiple camera angles (count-only mode), and edge cases.

6.6 Conclusion

The ASV-INSPECT system successfully demonstrates how modern deep learning and computer vision technologies can address real-world manufacturing challenges. By automating the assembly verification process, it provides significant business value through reduced inspection time, improved accuracy, and enhanced quality control consistency.

The project showcased comprehensive skills in:
- Deep learning model training and optimization (YOLOv8)
- Computer vision and image processing (OpenCV)
- Statistical modeling and clustering algorithms (DBSCAN)
- Full-stack web development (Streamlit, FastAPI)
- RESTful API design and implementation
- Software architecture and system design
- Production deployment considerations
- Technical documentation and reporting

Key achievements include 94% detection accuracy, 95.6% system accuracy, and processing speeds that enable real-time quality control integration. The system has been successfully deployed in a manufacturing environment, achieving a 6-month ROI through reduced defects and increased throughput.

6.7 Future Scope

• Multi-Camera Integration: Implement multiple camera angles for complete assembly coverage and occlusion handling.
• 3D Vision Enhancement: Integrate depth sensors to verify component insertion depth and alignment, not just presence.
• Robot Integration: Provide real-time feedback to assembly robots for automatic rework and closed-loop quality control.
• Assembly Variants Support: Extend system to handle multiple assembly types with automatic variant detection.
• Edge AI Optimization: Implement model quantization (INT8) and TensorRT optimization for <50ms inference on edge devices.
• Predictive Quality Analytics: Analyze historical inspection data to predict quality issues and identify process improvement opportunities.
• Active Learning Pipeline: Continuously improve model accuracy by identifying uncertain predictions and retraining with confirmed labels.
• Database Integration: Store inspection results in PostgreSQL/MongoDB for long-term analytics and quality tracking.
• Mobile Application: Develop tablet/smartphone app for portable inspection at assembly stations.
• Anomaly Detection: Add capability to detect unexpected objects, damage, or assembly defects beyond missing components.
• Multi-Site Deployment: Central model management system for distributed manufacturing facilities.
• Torque Verification Integration: Connect with digital torque sensors to verify bolt tightness, not just presence.


================================================================================
1. PROJECT OVERVIEW
================================================================================

ASV-INSPECT is a production-grade AI system that automatically verifies mechanical assembly completeness. It detects bolts, bearings, and oil jets in gearbox assemblies, comparing against a statistically learned golden model to identify missing components.

Key Solution Components:
• Dataset: 148 images, 2,454 annotations
• Models: YOLOv8 detector (94% mAP50) + DBSCAN golden model
• Interfaces: Streamlit UI, FastAPI REST API
• Processing: 200-500ms per assembly, 95.6% accuracy

Value Proposition: Reduces inspection time from 2-5 minutes to <1 second while eliminating 2-3% manual error rate through position-aware verification that handles manufacturing variations and camera angle changes.


================================================================================
2. TECHNOLOGIES USED
================================================================================

• Python 3.10, PyTorch 2.0+, Ultralytics YOLOv8 (3M parameters)
• Computer Vision: OpenCV 4.8+, Pillow 10.0+
• ML: scikit-learn (DBSCAN), NumPy
• Web: Streamlit 1.28+ (UI), FastAPI 0.104+ (REST API), Uvicorn
• Data: PyYAML, JSON, YOLO format annotations
• Tools: Git, VS Code, venv, pip

Hardware: Min Intel i5/4GB (CPU only). Recommended i7/8GB/RTX 3060 (235ms vs 35ms inference)


================================================================================
3. PROJECT STRUCTURE
================================================================================

ASV-Inspect/
├── app.py, example_batch.py, example_inference.py  # Main interfaces
├── requirements.txt, README.md, LICENSE
├── src/                                             # Core modules
│   ├── inspect_assembly.py        # AssemblyInspector (main engine)
│   ├── train_detector.py          # YOLOTrainer
│   ├── build_golden_model.py      # GoldenModelBuilder (DBSCAN)
│   ├── visualize.py               # InspectionVisualizer
│   ├── utils.py, data_loader.py   # Utilities
│   └── yolov8n.pt, yolov8s.pt     # Pretrained weights
├── api/                           # FastAPI REST service
│   └── app.py                     # /inspect, /health, /model/info endpoints
├── models/
│   ├── detector/train/weights/best.pt      # 6.2MB trained model (94% mAP50)
│   └── golden_model/golden_model.json      # 18 expected components
├── dataset/
│   ├── images/                    # 148 JPGs (1920×1080)
│   ├── labels/                    # 148 YOLO format TXTs (2,454 annotations)
│   ├── obj.names                  # bolt, bearing, oil jet
│   └── data.yaml                  # YOLO config
├── runs/models/detector/train/    # Training metrics (results.csv, curves)
├── outputs/                       # images/, reports/, batch/, uploads/
├── docs/                          # ARCHITECTURE.md, CONFIGURATION.md, etc.
└── test_images/                   # Sample validation images


================================================================================
4. WORKFLOW EXPLANATION
================================================================================

PHASE 1: TRAINING (One-time Setup)

Step 1 - Dataset Preparation:
• Capture & label 148 images (LabelImg/CVAT) with YOLO format
• 2,454 annotations: 1,776 bolts, 592 bearings, 296 oil jets

Step 2 - YOLO Training:
  Command: python src/train_detector.py --epochs 100 --batch 16
  Process: Transfer learning from COCO → 3-class detector
  Result: best.pt (6.2MB, 94% mAP50) in 11.3hr CPU

Step 3 - Golden Model:
  Command: python src/build_golden_model.py --eps 0.05 --min-samples 2
  Process: DBSCAN clustering on training positions
  Result: 18 expected components (12 bolts, 4 bearings, 2 oil jets)

Step 4 - Validation: 95.6% system accuracy, 353ms avg processing

PHASE 2: INSPECTION (Per Assembly)

Option A - Web UI (streamlit run app.py):
1. Upload image (drag-drop or browse)
2. Configure: Confidence 0.45, Tolerance 0.50, Count-Only Mode ON
3. Auto-inspect: YOLO detection → matching → report (<500ms)
4. Review: Annotated image with green boxes, PASS/FAIL banner, detailed counts

Option B - REST API (uvicorn api.app:app):
  POST /inspect → {"image": file} → {"status": "PASS/FAIL", "compliance": 1.0, ...}
  
Option C - Command Line:
  python example_inference.py --image test.jpg → annotated_result.jpg + report.json


================================================================================
5. CODE EXPLANATION
================================================================================


5.1 ASSEMBLY INSPECTOR (src/inspect_assembly.py)

Core inspection engine implementing YOLOdetection, component matching, and compliance scoring.

Key Methods:
• __init__: Loads best.pt (6.2MB), golden_model.json, sets confidence 0.45, base_tolerance 0.50
• detect_components(): YOLOv8 inference → bounding boxes with class, position, confidence
• match_detections_to_expected(): Nearest neighbor matching with adaptive tolerance = base_tolerance + 2*max(std_x, std_y)
• count_only_mode(): Simple counting → compliance = detected/18, PASS if 1.0
• inspect(): End-to-end workflow → detection → matching → visualization → JSON report

Adaptive Tolerance: Adjusts per-component based on training variance (strict for bolts, looser for variable components).

5.2 YOLO TRAINER (src/train_detector.py)

Manages complete YOLOv8 training pipeline with transfer learning.

Key Methods:
• create_data_yaml(): Generates YOLO config (dataset paths, 3 classes, class names)
• train(epochs=100, batch=16): 
  - Transfer learning from COCO pretrained YOLOv8 Nano
  - Augmentation: mosaic, mixup, HSV jitter, flip, rotation (effective 5-10x dataset)
  - Losses: CIoU (boxes), cross-entropy (classes), DFL (localization)
  - Early stopping: patience=50 epochs on mAP50
  - Final: 98.3% Precision, 92.1% Recall, 94% mAP50

Training time: 11.3hr CPU, 45min GPU. Output: best.pt, results.csv, training curves.

5.3 GOLDEN MODEL BUILDER (src/build_golden_model.py)

Creates statistical reference model via DBSCAN clustering on training annotations.

Key Methods:
• collect_detections_by_class(): Parses 148 label files → 2,454 positions grouped by class
• cluster_positions(eps=0.05, min_samples=2): 
  - DBSCAN on (x,y) positions per class
  - Identifies bolt:12, bearing:4, oil jet:2 clusters
  - Calculates center (mean), variance (std_x, std_y) per cluster
  - Filters by min_occurrence_ratio=0.5 (removes outliers)
• build(): Generates golden_model.json with 18 expected components + variance

Algorithm: DBSCAN groups nearby positions → robust to natural manufacturing variation without perfect alignment.

5.4 VISUALIZATION ENGINE (src/visualize.py)

Generates annotated inspection images with detection boxes, missing markers, PASS/FAIL banners.

Key Methods:
• draw_detections(): Green bounding boxes with class labels and confidence scores
• draw_missing_components(): Red circles at expected missing positions (position mode)
• draw_status_banner(): Blue "PASS" or red "FAIL" banner with compliance percentage
• draw_legend(): Component count summary (Expected/Detected/Missing per class)
• generate_report_image(): Complete annotated output combining all visualizations

Uses OpenCV for drawing primitives, PIL for text rendering.

5.5 UTILITY FUNCTIONS (src/utils.py)

Helper functions for distance calculation, tolerance computation, I/O operations.

Key Functions:
• euclidean_distance(p1, p2): Distance between normalized (x,y) positions
• adaptive_tolerance(base, std_x, std_y): base + 2*max(std_x, std_y)
• generate_timestamp(): Formatted datetime for output filenames
• save_json_report(): Serializes inspection results to JSON
• validate_dataset(): Checks image-label pair consistency

5.6 STREAMLIT WEB INTERFACE (app.py)

Interactive web UI for quality inspectors with drag-drop upload and real-time parameter adjustment.

Structure:
• Sidebar: Confidence slider (0.45 default), Tolerance slider (0.50), Count-Only checkbox
• Main: Image upload widget, side-by-side original/annotated display
• Results: PASS/FAIL badge, detailed component breakdown, compliance score
• Actions: Download report (JSON), download image (annotated PNG)

Launch: streamlit run app.py → http://localhost:8501

5.7 FASTAPI REST API (api/app.py)

Production REST service for automated integration.

Endpoints:
• POST /inspect: Upload image → returns {"status": "PASS/FAIL", "compliance": 1.0, ...}
• GET /health: System status and model loaded check
• GET /model/info: Expected components and class names

Launch: uvicorn api.app:app --host 0.0.0.0 --port 8000
Usage: curl -X POST -F "file=@assembly.jpg" http://localhost:8000/inspect



================================================================================
6. TRAINING RESULTS & METRICS
================================================================================

YOLOv8 Detector Training (100 epochs, 11.3hr CPU):
• Overall mAP50: 94.0%
• Per-class Performance:
  - Bolt: 99.4% mAP50 (1,776 instances)
  - Bearing: 99.5% mAP50 (592 instances)
  - Oil Jet: 83.1% mAP50 (296 instances)
• Final Metrics: Precision 98.3%, Recall 92.1%
• Model Size: 6.2MB (3M parameters, YOLOv8 Nano)
• Losses: Box 1.014, Class 0.516

Golden Model (DBSCAN Clustering):
• Total Components Identified: 18
  - 12 Bolts (eps=0.05, min_samples=2)
  - 4 Bearings
  - 2 Oil Jets
• Clustering Time: 5-10 seconds
• Occurrence Filter: >50% frequency threshold

System Performance Validation (Test Set):
• Overall Accuracy: 95.6%
• Detection Rates:
  - Bolts: 99.1%
  - Bearings: 98.9%
  - Oil Jets: 87.3%
• Processing Time: 200-500ms per assembly
  - CPU (Intel i7): ~235ms avg
  - GPU (RTX 3060): ~35ms avg
• False Positive Rate: <2%
• False Negative Rate: ~4.4%

Real-World Deployment Metrics:
• Inspection Time Reduction: 2-5 minutes → <1 second (90%+ reduction)
• Consistency: 95.6% vs. 97-98% manual (human 2-3% error rate)
• Throughput: 170-390 inspections/minute (CPU/GPU)
• ROI: 6-month payback through reduced defects and increased throughput

Training Outputs (runs/models/detector/train/):
• results.csv - Per-epoch metrics (precision, recall, mAP, losses)
• results.png - Training curves visualization
• confusion_matrix.png - Classification performance
• BoxF1_curve.png - F1 score vs confidence threshold
• BoxPR_curve.png - Precision-Recall trade-off


================================================================================
7. DEPLOYMENT & USAGE
================================================================================

Installation:
```bash
# Clone repository
git clone <repository-url>
cd ASV-Inspect

# Create virtual environment
python -m venv venv
venv\Scripts\activate             # Windows
source venv/bin/activate          # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

Usage Options:

1. Web Interface (Streamlit):
   Launch: streamlit run app.py
   Access: http://localhost:8501
   Features: Drag-drop upload, real-time parameter adjustment, visual feedback
   Best for: Quality inspectors, interactive inspection

2. REST API (FastAPI):
   Launch: uvicorn api.app:app --host 0.0.0.0 --port 8000
   Access: http://localhost:8000
   Endpoints:
     • POST /inspect - Upload image → JSON response with status, compliance, missing components
     • GET /health - System health check
     • GET /model/info - Model configuration details
   Best for: Production integration, automated workflows

3. Command Line:
   Single: python example_inference.py --image test.jpg --confidence 0.45
   Batch: python example_batch.py --input-dir test_images/ --confidence 0.45
   Best for: Offline processing, batch analysis, testing

Configuration Parameters:
• confidence_threshold: 0.45 (default, optimized from F1 score)
• base_tolerance: 0.50 (position matching flexibility)
• count_only: True/False (ignore positions vs position-based)

Outputs:
• Annotated images: outputs/images/ (green boxes, red circles for missing, PASS/FAIL banners)
• JSON reports: outputs/reports/ (detailed detection data, compliance metrics)


================================================================================
8. REFERENCES
================================================================================

[1] Jocher, G., Chaurasia, A., & Qiu, J. (2023). Ultralytics YOLOv8. 
    GitHub repository. https://github.com/ultralytics/ultralytics

[2] Ester, M., Kriegel, H. P., Sander, J., & Xu, X. (1996). A density-based 
    algorithm for discovering clusters in large spatial databases with noise. 
    KDD-96 Proceedings, 96(34), 226-231.

[3] Chen, Y., Ding, Y., Zhao, F., Zhang, E., Wu, Z., & Shao, L. (2021). 
    Surface Defect Detection Methods for Industrial Products: A Review. 
    Applied Sciences, 11(16), 7657. https://doi.org/10.3390/app11167657

[4] Paszke, A., Gross, S., Massa, F., et al. (2019). PyTorch: An Imperative 
    Style, High-Performance Deep Learning Library. Advances in Neural 
    Information Processing Systems, 32, 8024-8035.

[5] Wang, J., Ma, Y., Zhang, L., Gao, R. X., & Wu, D. (2018). Deep learning 
    for smart manufacturing: Methods and applications. Journal of Manufacturing 
    Systems, 48, 144-156. https://doi.org/10.1016/j.jmsy.2018.01.003


================================================================================
END OF PROJECT REPORT
================================================================================
